#!/usr/bin/env python3
"""Restore git repos from a Google Drive *backup snapshot* into local ~/Projects.

Context
- A shared-drive folder contains a backup snapshot (e.g. "Backup/Petr.local/Users/premiumgastro").
- GitHub Desktop can get confused and point at those backup paths.
- This tool copies actual repos out of the backup snapshot into a clean local folder.

Safety
- DRY RUN by default.
- With --apply: COPY only (rsync). Source backup stays intact.
- Never deletes anything.

Dedup strategy
- If a repo has the same origin+HEAD and both are clean, we skip copying.
- Otherwise we copy into the recovery destination for later manual consolidation.

Outputs
- A TSV report + JSONL log in ops/_local/dev_audit (gitignored).
"""

from __future__ import annotations

import argparse
import csv
import gzip
import json
import os
import subprocess
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple


def _now_ts() -> str:
    return time.strftime("%Y%m%d-%H%M%S")


def _run(cmd: List[str], cwd: Optional[str] = None, timeout: int = 20) -> Tuple[int, str, str]:
    p = subprocess.run(
        cmd,
        cwd=cwd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        timeout=timeout,
    )
    return p.returncode, p.stdout, p.stderr


def _git(repo: Path, args: List[str], timeout: int = 20) -> Tuple[int, str, str]:
    return _run(["git", "-C", str(repo)] + args, timeout=timeout)


@dataclass(frozen=True)
class RepoState:
    origin: str
    head: str
    dirty_files: int


def _repo_state(repo: Path) -> RepoState:
    origin = ""
    rc, out, _ = _git(repo, ["config", "--get", "remote.origin.url"], timeout=10)
    if rc == 0:
        origin = out.strip()

    head = ""
    rc, out, _ = _git(repo, ["rev-parse", "--short", "HEAD"], timeout=10)
    if rc == 0:
        head = out.strip()

    dirty_files = 0
    rc, out, _ = _git(repo, ["status", "--porcelain"], timeout=20)
    if rc == 0:
        dirty_files = len([ln for ln in out.splitlines() if ln.strip()])

    return RepoState(origin=origin, head=head, dirty_files=dirty_files)


def _load_local_inventory(tsv_path: Path) -> Dict[str, RepoState]:
    """origin -> RepoState"""
    by_origin: Dict[str, RepoState] = {}
    if not tsv_path.exists():
        return by_origin

    with tsv_path.open("r", encoding="utf-8", newline="") as f:
        r = csv.DictReader(f, delimiter="\t")
        for row in r:
            origin = (row.get("origin") or "").strip()
            head = (row.get("head") or "").strip()
            try:
                dirty = int((row.get("dirty_files") or "0").strip())
            except Exception:
                dirty = 0
            if origin:
                by_origin[origin] = RepoState(origin=origin, head=head, dirty_files=dirty)
    return by_origin


def _iter_backup_repo_roots(fsmap_gz: Path, backup_prefix: str) -> List[Path]:
    roots: List[str] = []
    seen = set()
    with gzip.open(fsmap_gz, "rt", encoding="utf-8", errors="replace") as f:
        for line in f:
            if ".git" not in line:
                continue
            try:
                rec = json.loads(line)
            except Exception:
                continue
            p = rec.get("path")
            if not isinstance(p, str) or not p.endswith("/.git"):
                continue
            if not p.startswith(backup_prefix):
                continue
            root = p[:-len("/.git")]
            if root in seen:
                continue
            seen.add(root)
            roots.append(root)
    roots.sort()
    return [Path(r) for r in roots]


def _rel_from_backup(root: Path, backup_prefix: str) -> str:
    s = str(root)
    if not s.startswith(backup_prefix):
        return ""
    rel = s[len(backup_prefix):]
    return rel.lstrip("/")


def _is_dot_top(rel: str) -> bool:
    top = rel.split("/", 1)[0] if rel else ""
    return top.startswith(".")


def _rsync_copy(src: Path, dest: Path) -> Tuple[int, str, str]:
    dest.parent.mkdir(parents=True, exist_ok=True)
    # openrsync: -a implies -rlptgoD (good enough)
    return _run(["rsync", "-a", f"{src}/", str(dest)], timeout=60 * 30)


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--fsmap",
        default="",
        help="Path to FS_MAP.jsonl.gz (generated by full_fs_map.py).",
    )
    ap.add_argument(
        "--backup-prefix",
        default="",
        help="Prefix ending with .../Backup/Petr.local/Users/premiumgastro/",
    )
    ap.add_argument(
        "--local-inventory-tsv",
        default="",
        help="dev_repo_inventory.py TSV for ~/Projects (used for dedup decisions)",
    )
    ap.add_argument(
        "--dest-root",
        default="",
        help="Destination root under ~/Projects (default: ~/Projects/99-Legacy/RESTORE__GDriveBackup__YYYY-MM-DD)",
    )
    ap.add_argument("--max", type=int, default=0, help="Optional max repos to process (0 = no limit)")
    ap.add_argument("--apply", action="store_true", help="Actually copy repos (default: dry-run)")
    args = ap.parse_args()

    fsmap = Path(args.fsmap).expanduser() if args.fsmap else Path.cwd() / "FSMapSnapshot-20260208-111427" / "FS_MAP.jsonl.gz"
    if not fsmap.exists():
        print(f"ERROR: fsmap not found: {fsmap}")
        return 2

    backup_prefix = args.backup_prefix or (
        "/Users/premiumgastro/Library/CloudStorage/GoogleDrive-ps@premium-gastro.com/"
        "Sdílené disky/PG/pspg/Backup/Petr.local/Users/premiumgastro/"
    )

    local_inv_path = Path(args.local_inventory_tsv).expanduser() if args.local_inventory_tsv else None
    by_origin = _load_local_inventory(local_inv_path) if local_inv_path else {}

    dest_root = Path(args.dest_root).expanduser() if args.dest_root else (
        Path.home() / "Projects/99-Legacy" / f"RESTORE__GDriveBackup__{time.strftime('%Y-%m-%d')}"
    )

    report_dir = Path("/Users/premiumgastro/Projects/00-Premium-Gastro/premium-gastro-ai-assistant/ops/_local/dev_audit")
    report_dir.mkdir(parents=True, exist_ok=True)
    ts = _now_ts()
    tsv_out = report_dir / f"restore_gdrive_backup_repos_{ts}.tsv"
    jsonl_out = report_dir / f"restore_gdrive_backup_repos_{ts}.jsonl"

    roots = _iter_backup_repo_roots(fsmap, backup_prefix)
    if args.max and args.max > 0:
        roots = roots[: args.max]

    # Filter out dot-top repos by default; keep them in the report but do not restore.
    rows: List[Dict[str, str]] = []
    restored = 0
    skipped = 0
    ignored = 0
    errors = 0

    for src_root in roots:
        rel = _rel_from_backup(src_root, backup_prefix)
        dot_top = 1 if _is_dot_top(rel) else 0
        state = RepoState(origin="", head="", dirty_files=0)
        try:
            state = _repo_state(src_root)
        except Exception as e:
            errors += 1
            rec = {
                "ts": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "action": "inspect",
                "src": str(src_root),
                "ok": False,
                "error": f"{type(e).__name__}: {e}",
            }
            jsonl_out.parent.mkdir(parents=True, exist_ok=True)
            with jsonl_out.open("a", encoding="utf-8") as jf:
                jf.write(json.dumps(rec, ensure_ascii=True) + "\n")
            rows.append({
                "src": str(src_root),
                "rel": rel,
                "origin": "",
                "head": "",
                "dirty_files": "0",
                "decision": "error",
                "dest": "",
                "note": rec["error"],
            })
            continue

        decision = "restore"
        note = ""
        dest = ""

        if dot_top:
            decision = "ignore_dot_top"
            note = "Starts with dot; not restoring automatically"
            ignored += 1
        else:
            local = by_origin.get(state.origin) if state.origin else None
            if local and local.head == state.head and local.dirty_files == 0 and state.dirty_files == 0:
                decision = "skip_duplicate"
                note = "Origin+HEAD match and both clean"
                skipped += 1
            else:
                # Restore under dest_root, preserving relative path to keep uniqueness.
                dest_path = dest_root / rel
                # Avoid accidental overwrite.
                if dest_path.exists():
                    for i in range(1, 10_000):
                        alt = dest_root / f"{rel}__restored__{i}"
                        if not alt.exists():
                            dest_path = alt
                            break
                dest = str(dest_path)

                if args.apply:
                    rc, out, err = _rsync_copy(src_root, Path(dest))
                    ok = rc == 0 and Path(dest).is_dir()
                    rec = {
                        "ts": time.strftime("%Y-%m-%dT%H:%M:%S"),
                        "action": "copy",
                        "src": str(src_root),
                        "dest": dest,
                        "ok": ok,
                        "rc": rc,
                        "stderr": (err or "").strip()[:500],
                    }
                    with jsonl_out.open("a", encoding="utf-8") as jf:
                        jf.write(json.dumps(rec, ensure_ascii=True) + "\n")
                    if ok:
                        restored += 1
                        decision = "restored"
                    else:
                        errors += 1
                        decision = "restore_failed"
                        note = (err or out or "").strip()[:200]
                else:
                    decision = "would_restore"

        rows.append({
            "src": str(src_root),
            "rel": rel,
            "origin": state.origin,
            "head": state.head,
            "dirty_files": str(state.dirty_files),
            "decision": decision,
            "dest": dest,
            "note": note,
        })

    # Write TSV report.
    with tsv_out.open("w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()), delimiter="\t")
        w.writeheader()
        w.writerows(rows)

    print("FSMap:", fsmap)
    print("Backup prefix:", backup_prefix)
    print("Dest root:", dest_root)
    print("Apply:", bool(args.apply))
    print("Repos scanned:", len(roots))
    print("Restored:", restored)
    print("Skipped (dedup):", skipped)
    print("Ignored (dot-top):", ignored)
    print("Errors:", errors)
    print("Report TSV:", tsv_out)
    print("Log JSONL:", jsonl_out)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
